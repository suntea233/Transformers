{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:37<05:36, 37.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7588, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:14<05:00, 37.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8137, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:52<04:23, 37.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4237, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:30<03:45, 37.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1544, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:08<03:08, 37.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.9464, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:45<02:30, 37.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7600, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:23<01:53, 37.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5670, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [05:01<01:15, 37.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3860, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:39<00:37, 37.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2031, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:16<00:00, 37.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0372, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "from Datasets import Datasets\n",
    "\n",
    "dataset = Datasets(\"C:\\Attention\\data\\\\train.txt\")\n",
    "\n",
    "dataset.bulid_vocab(dataset.en_data,dataset.ch_data)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, num_workers=0,collate_fn=dataset.collate_fn)\n",
    "\n",
    "\n",
    "maxlen = 128\n",
    "d_model = 512\n",
    "units = 512\n",
    "dropout_rate = 0.2\n",
    "numofblock = 4\n",
    "numofhead = 4\n",
    "# encoder_vocab = len(dataset.ch_vocab)\n",
    "vocab_size = len(dataset.en_vocab)\n",
    "epochs = 10\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_padding_mask(seq_q,seq_k):\n",
    "    # print(seq_k.shape)\n",
    "    # print(seq_q.shape)\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    padding_mask = seq_k.data.eq(1).unsqueeze(1)\n",
    "    return padding_mask.expand(batch_size,len_q,len_k)\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,emb_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "\n",
    "        return self.embedding(x).to(DEVICE) # shape = (batch_size,input_seq_len,emb_dim)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_units, num_heads, dropout_rate, mask=False):\n",
    "        super().__init__()\n",
    "        self.num_units = num_units\n",
    "        self.num_head = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = mask\n",
    "        self.linearQ = nn.Linear(self.num_units,self.num_units)\n",
    "        self.linearK = nn.Linear(self.num_units,self.num_units)\n",
    "        self.linearV = nn.Linear(self.num_units,self.num_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.LayerNormalization = nn.LayerNorm(d_model)\n",
    "        self.Q = nn.Sequential(self.linearQ,self.relu)\n",
    "        self.K = nn.Sequential(self.linearK,self.relu)\n",
    "        self.V = nn.Sequential(self.linearV,self.relu)\n",
    "\n",
    "\n",
    "    def forward(self, queries, keys, values, self_padding_mask, enc_dec_padding_mask):\n",
    "        '''\n",
    "        :param queries: shape:(batch_size,input_seq_len,d_model)\n",
    "        :param keys: shape:(batch_size,input_seq_len,d_model)\n",
    "        :param values: shape:(batch_size,input_seq_len,d_model)\n",
    "        :return: None\n",
    "        '''\n",
    "        q, k, v = self.Q(queries), self.K(keys), self.V(values)\n",
    "\n",
    "        q_split, k_split, v_split = torch.chunk(q,self.num_head,dim=-1), torch.chunk(k,self.num_head,dim=-1), torch.chunk(v,self.num_head,dim=-1)\n",
    "        q_, k_, v_ = torch.stack(q_split,dim=1), torch.stack(k_split,dim=1), torch.stack(v_split,dim=1)\n",
    "        # shape : (batch_size, num_head, input_seq_len, depth = d_model/num_head)\n",
    "        a = torch.matmul(q_,k_.permute(0,1,3,2)) # a = q * k^T(后两个维度)\n",
    "        a = a / (k_.size()[-1] ** 0.5) # shape:(batch_size,num_head,seq_len,seq_len)\n",
    "        batch_size_shape = a.shape[0]\n",
    "        seq_len_shape = a.shape[2]\n",
    "        if self.mask:\n",
    "            self_padding_mask = self_padding_mask.unsqueeze(1).repeat(1, self.num_head, 1, 1)\n",
    "            masked = torch.ones((batch_size_shape,1,seq_len_shape,seq_len_shape))\n",
    "            masked = Variable((1 - torch.tril(masked, diagonal=0)) * (-2 ** 32 + 1)).to(DEVICE)\n",
    "\n",
    "            assert masked.shape[-1] == self_padding_mask.shape[-1]\n",
    "            a = a + masked\n",
    "            a.masked_fill_(self_padding_mask,-1e9)\n",
    "        else:\n",
    "            enc_dec_padding_mask = enc_dec_padding_mask.unsqueeze(1).repeat(1, self.num_head, 1, 1)\n",
    "            a.masked_fill_(enc_dec_padding_mask,-1e9)\n",
    "\n",
    "        a = F.softmax(a,dim=-1)\n",
    "\n",
    "        a = torch.matmul(a,v_)\n",
    "        a = torch.reshape(a.permute(0, 2, 1, 3), shape=(q.shape[0],q.shape[1],q.shape[2]))\n",
    "        a = self.dropout(a)\n",
    "        a += queries\n",
    "        a = self.LayerNormalization(a).to(DEVICE)\n",
    "        return a\n",
    "\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self,input_channels,units=(2048,512)):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.units = units\n",
    "        self.layer1 = nn.Linear(self.input_channels,units[0])\n",
    "        self.layer2 = nn.Linear(self.units[0],self.units[1])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.LayerNormalization = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        outputs = self.layer1(x)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.layer2(outputs)\n",
    "        outputs += x\n",
    "        outputs = self.LayerNormalization(outputs)\n",
    "        return outputs.to(DEVICE)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [seq_len, batch_size, d_model]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mask_self_attention = MultiHeadAttention(units,numofhead,dropout_rate,True)\n",
    "        self.fc = FC(d_model)\n",
    "\n",
    "    def forward(self,inputs,padding_mask):\n",
    "        outputs = self.mask_self_attention(inputs,inputs,inputs,padding_mask,None)\n",
    "        outputs = self.fc(outputs)\n",
    "        return outputs.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(units,numofhead,dropout_rate,mask=False)\n",
    "        self.fc = FC(d_model)\n",
    "\n",
    "    def forward(self,enc_outputs,padding_mask):\n",
    "        # enc_outputs = self.self_attention(enc_outputs,enc_outputs,enc_outputs,None,padding_mask)\n",
    "        outputs = self.fc(enc_outputs)\n",
    "        return outputs.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(numofblock)])\n",
    "\n",
    "\n",
    "    def forward(self,x,padding_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,padding_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pe = PositionalEncoding()\n",
    "        self.embedding = TokenEmbedding(vocab_size,units)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(numofblock)])\n",
    "\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs = self.pe(outputs.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        padding_mask = get_padding_mask(inputs,inputs)\n",
    "        for layer in self.layers:\n",
    "            outputs = layer(outputs,padding_mask)\n",
    "        return outputs,padding_mask\n",
    "\n",
    "\n",
    "\n",
    "class CTG(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(CTG, self).__init__()\n",
    "        self.Encoder = Encoder(vocab_size)\n",
    "        self.Decoder = Decoder()\n",
    "        self.linear = nn.Linear(d_model,vocab_size)\n",
    "\n",
    "    def forward(self,x,epoch=None):\n",
    "        enc_outputs,padding_mask = self.Encoder(x)\n",
    "        # print(enc_outputs.shape)\n",
    "        enc_outputs = self.Decoder(enc_outputs,padding_mask)\n",
    "        logits = self.linear(enc_outputs)\n",
    "        # if epoch == 9:\n",
    "        #     print(logits)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        return logits\n",
    "\n",
    "model = CTG(vocab_size).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
    "# #\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    total = []\n",
    "    for _,dec_inputs,dec_outputs in dataloader:\n",
    "\n",
    "        dec_inputs,dec_outputs= dec_inputs.to(DEVICE),dec_outputs.to(DEVICE)\n",
    "        # for i in dec_inputs:\n",
    "        #     print(dataset.idx2enwords(i))\n",
    "        outputs = model(dec_inputs,epoch)\n",
    "\n",
    "        loss = criterion(outputs,dec_outputs.contiguous().view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        total.append(loss)\n",
    "        optimizer.step()\n",
    "    print(sum(total)/len(total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't have the address now.\n",
      "i don't have to do it right away.\n",
      "i don't think that she was a little cold.\n",
      "i don't have a car.\n",
      "i don't have the slightest idea.\n",
      "i don't have the slightest idea.\n",
      "i don't think that she will do this.\n",
      "i don't think that she can speak english.\n",
      "i don't have a car.\n",
      "i don't have a car.\n",
      "i don't have the slightest idea.\n",
      "i don't have a car.\n",
      "i don't think that can do any good.\n",
      "i don't think that he will do it right away.\n",
      "i don't have a car.\n",
      "i don't think that can help you.\n",
      "i don't think that she can speak english.\n",
      "i don't have a car.\n",
      "i don't think that can do any good.\n",
      "i don't think that she can do it.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def greedy_decoder(model, start_symbol):\n",
    "    \"\"\"贪心编码\n",
    "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
    "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
    "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
    "    :param model: Transformer Model\n",
    "    :param enc_input: The encoder input\n",
    "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 4\n",
    "    :return: The target input\n",
    "    \"\"\"\n",
    "    inputs = torch.zeros(1, 0).long()\n",
    "    terminal = False\n",
    "    next_symbol = start_symbol\n",
    "    while not terminal:\n",
    "        # 预测阶段：inputs序列会一点点变长（每次添加一个新预测出来的单词）\n",
    "        inputs = torch.cat([inputs.to(DEVICE), torch.tensor([[next_symbol]], dtype=inputs.dtype).to(DEVICE)],\n",
    "                              -1)\n",
    "        # print(\"inputs:\")\n",
    "        # print(inputs)\n",
    "        dec_outputs,_ = model.Encoder(inputs)\n",
    "        dec_outputs = model.Decoder(dec_outputs,_)\n",
    "        dec_outputs = model.linear(dec_outputs)\n",
    "        # projected = model.linear(dec_outputs)\n",
    "        prob = dec_outputs.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        # print(\"prob:\")\n",
    "        # print(dataset.idx2enwords(prob))\n",
    "        # 增量更新（我们希望重复单词预测结果是一样的）\n",
    "        # 我们在预测是会选择性忽略重复的预测的词，只摘取最新预测的单词拼接到输入序列中\n",
    "        next_word = prob.data[-1]  # 拿出当前预测的单词(数字)。我们用x'_t对应的输出z_t去预测下一个单词的概率，不用z_1,z_2..z_{t-1}\n",
    "        next_symbol = next_word\n",
    "        # print(dataset.idx2en(next_word))\n",
    "        if next_symbol == dataset.en_vocab[\"<eos>\"]:\n",
    "            terminal = True\n",
    "        # print(next_word)\n",
    "\n",
    "    # greedy_dec_predict = torch.cat(\n",
    "    #     [inputs.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],\n",
    "    #     -1)\n",
    "    greedy_dec_predict = inputs[:, 1:]\n",
    "    return greedy_dec_predict\n",
    "\n",
    "for i in range(20):\n",
    "    greedy_dec_predict = greedy_decoder(model, start_symbol=dataset.en_vocab[\"<bos>\"])\n",
    "    # print(input[i], '->', greedy_dec_predict.squeeze())\n",
    "    print(\" \".join([dataset.idx2en(n.item()) for n in greedy_dec_predict.squeeze()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = 35\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        # print(x_cond)\n",
    "        logits = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        # logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        # print(dataset.idx2enwords(x[-1]))\n",
    "        ix = ix[-1]\n",
    "        # print(ix.shape)\n",
    "        # print(dataset.idx2enwords(ix))\n",
    "        ix = ix.unsqueeze(0)\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> anyone can do it? <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> on. <eos> something to page several books. <eos> something for a year due to the 98 the improve\n"
     ]
    }
   ],
   "source": [
    "x = dataset.words2idx(\"<bos> anyone can do\".split(),'en').unsqueeze(0).to(DEVICE)\n",
    "# x = torch.tensor([2,3,4], dtype=torch.long)[None, ...].to(DEVICE) # context conditioning\n",
    "y = sample(model, x, steps=30, temperature=1.0, sample=True, top_k=None)[0]\n",
    "print(dataset.idx2enwords(y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> a car consists of people.\n",
      "<bos> a doctor wouldn't do not make sense.\n",
      "<bos> a prince has rose in the traffic accident.\n",
      "<bos> a sudden out of four books have failed.\n",
      "<bos> a famous slightly and some me?\" the politician.\n",
      "<bos> a fire broke out after the drowsy after the island.\n",
      "<bos> a walk died.\n",
      "<bos> a bird in my team air folded the park. you have to see you think.\n",
      "<bos> a beautiful comic book judge\n",
      "<bos> a pity of air curtain.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_sequence():\n",
    "    s = dataset.words2idx(\"<bos> a\".split(),\"en\")\n",
    "    s = s.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # print(s.shape)\n",
    "    flag = True\n",
    "    data = torch.tensor([]).long().to(DEVICE).unsqueeze(0)\n",
    "    count = 0\n",
    "    while flag:\n",
    "        # print(s,data)\n",
    "        s = torch.cat((s,data),dim=-1)\n",
    "        dec_outputs = model(s.to(DEVICE),1)\n",
    "        prob = F.softmax(dec_outputs, dim=-1)\n",
    "        # prob = dec_outputs.squeeze(0)\n",
    "        prob = torch.multinomial(prob, num_samples=1)\n",
    "        # print(prob)\n",
    "        data = prob[-1].unsqueeze(0)\n",
    "\n",
    "\n",
    "#         print(data)\n",
    "        count += 1\n",
    "        if data == 3:\n",
    "            flag = False\n",
    "        if count == 20:\n",
    "            flag = False\n",
    "    # print()\n",
    "\n",
    "    # print(prob)\n",
    "#     print(s)\n",
    "    # for i in prob:\n",
    "    print(dataset.idx2enwords(s[-1]))\n",
    "for i in range(10):\n",
    "    get_sequence()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}